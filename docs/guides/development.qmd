---
title: Development Guide
date-modified: last-modified
---

Guide for developers contributing to or extending the omeka2dsp system.

## Development Environment Setup {#development-environment-setup}

We recommend using [**GitHub Codespaces**](https://github.com/features/codespaces) for development.

::: {.panel-tabset}

### 🚀 Reproducible Setup with GitHub Codespaces (Most Users)

1.  **Fork** this repository to your GitHub account.

2.  Click the green **`<> Code`** button and select **"Codespaces"**.

3.  Click **"Create codespace on `main`"**. This provides:

    -   ✅ Python with `uv`
    -   ✅ Node.js with `pnpm`
    -   ✅ Pre-configured development environment
    -   ✅ All dependencies pre-installed

4.  Create your development configuration:

    ``` bash
    # Create development environment file
    cp example.env .env.dev
    # Edit with your test instance credentials
    ```

5.  Create a development branch:

    ``` bash
    git checkout -b feature/your-feature-name
    ```

### 👩‍💻 Local Installation (Advanced)

#### Prerequisites

-   [uv (Python manager)](https://github.com/astral-sh/uv#installation)
-   [pnpm](https://pnpm.io/installation)
-   [Node.js](https://nodejs.org/en/download/) (latest LTS)
-   Git
-   Access to Omeka and DSP test instances

#### Initial Setup

``` bash
# Clone repository
git clone https://github.com/Stadt-Geschichte-Basel/omeka2dsp.git
cd omeka2dsp

# Create development branch
git checkout -b feature/your-feature-name

# Setup Python environment with dev dependencies
uv sync --dev

# Install Node.js development tools
pnpm install
pnpm run prepare
```

#### Development Configuration

Create a development environment file:

``` bash
cp example.env .env.dev
```

Edit `.env.dev` with test instance credentials:

``` bash
# Development configuration
OMEKA_API_URL=https://test-omeka.example.com/api/
KEY_IDENTITY=test_key_identity
KEY_CREDENTIAL=test_key_credential
ITEM_SET_ID=test_collection_id

# DSP test instance
PROJECT_SHORT_CODE=TEST
API_HOST=https://test-api.dasch.swiss
INGEST_HOST=https://test-ingest.dasch.swiss
DSP_USER=test@example.com
DSP_PWD=test_password

# Development settings
DEBUG_MODE=true
LOG_LEVEL=DEBUG
```

:::

## Code Architecture {#code-architecture}

### Module Structure

```         
scripts/
├── data_2_dasch.py              # Main migration orchestrator
├── process_data_from_omeka.py   # Omeka API interface
├── api_get_project.py           # DSP project utilities
├── api_get_lists.py            # DSP lists utilities
└── api_get_lists_detailed.py   # Detailed list utilities
```

Also refer to the documentation on the [Pipeline Architecture](/docs/architecture/index.qmd).

### Key Design Patterns

1.  **Repository Pattern**: Data access abstraction
2.  **Builder Pattern**: Payload construction
3.  **Strategy Pattern**: Different resource types
4.  **Command Pattern**: Update operations

### Core Components

```{mermaid}
classDiagram
    class MigrationOrchestrator {
        +main()
        +process_items()
        +handle_item()
    }
    
    class OmekaRepository {
        +get_items_from_collection()
        +get_media()
        +extract_property()
    }
    
    class DSPRepository {
        +login()
        +get_project()
        +get_resource_by_id()
        +create_resource()
    }
    
    class PayloadBuilder {
        +construct_payload()
        +map_properties()
        +extract_list_values()
    }
    
    class SynchronizationService {
        +check_values()
        +sync_resource()
        +update_value()
    }
    
    MigrationOrchestrator --> OmekaRepository
    MigrationOrchestrator --> DSPRepository
    MigrationOrchestrator --> PayloadBuilder
    MigrationOrchestrator --> SynchronizationService
```

### Data Flow

```{mermaid}
sequenceDiagram
    participant Main as main()
    participant Omeka as OmekaRepository
    participant Builder as PayloadBuilder
    participant DSP as DSPRepository
    participant Sync as SyncService
    
    Main->>Omeka: get_items_from_collection()
    Omeka->>Main: items[]
    
    loop For each item
        Main->>Omeka: get_media(item_id)
        Omeka->>Main: media[]
        
        Main->>DSP: get_resource_by_id()
        DSP->>Main: existing_resource or None
        
        alt Resource doesn't exist
            Main->>Builder: construct_payload()
            Builder->>Main: payload
            Main->>DSP: create_resource(payload)
        else Resource exists
            Main->>Sync: check_values()
            Sync->>Main: changes[]
            Main->>Sync: apply_updates()
        end
    end
```

Also refer to the documentation on the [Pipeline Workflow](/docs/workflows/index.qmd).

## Contributing Guidelines {#contributing-guidelines}

### Git Workflow

1.  **Create Feature Branch**

    ``` bash
    git checkout -b feature/description-of-feature
    ```

2.  **Make Changes**

    ``` bash
    # Make your changes
    git add .
    git commit -m "feat: add new synchronization feature"
    ```

3.  **Update Documentation**

    ``` bash
    # Update relevant documentation
    # Add tests for new features
    ```

4.  **Submit Pull Request**

    ``` bash
    git push origin feature/description-of-feature
    # Create PR on GitHub
    ```

### Commit Message Convention

Follow [Conventional Commits](https://www.conventionalcommits.org/):

``` bash
# Types
feat: new feature
fix: bug fix
docs: documentation changes
style: code style changes
refactor: code refactoring
test: test additions/changes
chore: maintenance tasks

# Examples
feat: add support for video files
fix: handle missing media files gracefully
docs: update API documentation
refactor: extract payload building to separate module
```

### Code Review Checklist

-   [ ] Code follows style guidelines
-   [ ] Tests are included and pass
-   [ ] Documentation is updated
-   [ ] No sensitive data in commits
-   [ ] Performance impact considered
-   [ ] Error handling included
-   [ ] Logging appropriate

## Testing {#testing}

### Test Structure

```         
tests/
├── unit/
│   ├── test_omeka_repository.py
│   ├── test_payload_builder.py
│   └── test_sync_service.py
├── integration/
│   ├── test_api_integration.py
│   └── test_end_to_end.py
└── fixtures/
    ├── sample_omeka_data.json
    └── sample_dsp_response.json
```

### Unit Testing

Create unit tests for individual components:

``` python
# tests/unit/test_payload_builder.py
import unittest
from unittest.mock import Mock
from scripts.data_2_dasch import construct_payload

class TestPayloadBuilder(unittest.TestCase):
    
    def setUp(self):
        self.sample_omeka_item = {
            "dcterms:title": [{"@value": "Test Title"}],
            "dcterms:identifier": [{"@value": "test123"}]
        }
        self.sample_lists = []
    
    def test_construct_basic_payload(self):
        """Test basic payload construction"""
        payload = construct_payload(
            self.sample_omeka_item,
            "sgb_OBJECT",
            "project_iri",
            self.sample_lists,
            "",
            ""
        )
        
        self.assertIn("@context", payload)
        self.assertIn("@type", payload)
        self.assertIn("rdfs:label", payload)
        self.assertEqual(payload["rdfs:label"], "Test Title")
    
    def test_handle_missing_title(self):
        """Test handling of missing required fields"""
        item_no_title = {"dcterms:identifier": [{"@value": "test123"}]}
        
        payload = construct_payload(
            item_no_title,
            "sgb_OBJECT", 
            "project_iri",
            self.sample_lists,
            "",
            ""
        )
        
        # Should handle gracefully
        self.assertIn("rdfs:label", payload)
```

### Integration Testing

Test API integrations:

``` python
# tests/integration/test_api_integration.py
import unittest
import os
from scripts.data_2_dasch import login, get_project

class TestAPIIntegration(unittest.TestCase):
    
    def setUp(self):
        self.dsp_user = os.getenv('TEST_DSP_USER')
        self.dsp_pwd = os.getenv('TEST_DSP_PWD')
    
    @unittest.skipIf(not os.getenv('TEST_DSP_USER'), "Test credentials not provided")
    def test_dsp_authentication(self):
        """Test DSP API authentication"""
        token = login(self.dsp_user, self.dsp_pwd)
        self.assertIsNotNone(token)
        self.assertIsInstance(token, str)
        self.assertGreater(len(token), 10)
    
    def test_project_retrieval(self):
        """Test project information retrieval"""
        project_iri = get_project()
        self.assertIsNotNone(project_iri)
        self.assertTrue(project_iri.startswith('http'))
```

### Running Tests

``` bash
# Run all tests
uv run pytest tests/

# Run specific test file
uv run pytest tests/unit/test_payload_builder.py

# Run with coverage
uv add --dev pytest-cov
uv run pytest --cov=scripts tests/

# Run integration tests
uv run pytest tests/integration/ -v
```

## Extending the System {#extending-the-system}

### Adding New Resource Types

1.  **Define Resource Class**

    ``` python
    # In construct_payload function
    if type == f"{PREFIX}sgb_NEW_TYPE":
        payload["specific_field"] = {
            "@type": "knora-api:TextValue",
            "knora-api:valueAsString": extract_property(item.get("custom:field", []), 42)
        }
    ```

2.  **Update Media Class Mapping**

    ``` python
    def specify_mediaclass(media_type: str) -> str:
        if media_type.startswith("audio/"):
            return f"{PREFIX}sgb_MEDIA_AUDIO"
        # ... existing mappings
    ```

3.  **Add Property Mappings**

    ``` python
    # Add new property extractors
    def extract_custom_property(item, prop_name):
        # Custom extraction logic
        pass
    ```

### Adding New Data Sources

Create new repository classes:

``` python
# scripts/new_data_source.py
class NewDataSourceRepository:
    
    def __init__(self, api_url, credentials):
        self.api_url = api_url
        self.credentials = credentials
    
    def get_items(self, collection_id):
        """Fetch items from new data source"""
        # Implementation here
        pass
    
    def transform_to_omeka_format(self, item):
        """Transform to standardized format"""
        # Convert to Omeka-like structure
        pass
```

### Custom Property Extractors

Add specialized extraction logic:

``` python
def extract_geo_coordinates(props):
    """Extract geographic coordinates"""
    for prop in props:
        value = prop.get('@value', '')
        if ',' in value:
            lat, lon = value.split(',')
            return {
                'latitude': float(lat.strip()),
                'longitude': float(lon.strip())
            }
    return None

def extract_date_range(props):
    """Extract date ranges"""
    for prop in props:
        value = prop.get('@value', '')
        if '-' in value:
            start, end = value.split('-')
            return {
                'start_date': start.strip(),
                'end_date': end.strip()
            }
    return None

# Register custom extractors
CUSTOM_EXTRACTORS = {
    'geo:coordinates': extract_geo_coordinates,
    'custom:dateRange': extract_date_range
}
```

### Plugin Architecture

Create extensible plugin system:

``` python
# scripts/plugins/base_plugin.py
class BasePlugin:
    
    def __init__(self, config):
        self.config = config
    
    def pre_process_item(self, item):
        """Called before item processing"""
        return item
    
    def post_process_item(self, item, result):
        """Called after item processing"""
        return result
    
    def transform_property(self, property_name, value):
        """Transform specific properties"""
        return value

# scripts/plugins/image_plugin.py
class ImageProcessingPlugin(BasePlugin):
    
    def pre_process_item(self, item):
        """Add image metadata"""
        media = self.extract_image_media(item)
        if media:
            item['image_metadata'] = self.extract_image_info(media)
        return item
    
    def extract_image_info(self, media_url):
        """Extract image dimensions, format, etc."""
        # Implementation here
        pass
```

## Debugging {#debugging}

### Debugging Configuration

``` python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Add debug prints
def debug_item_processing(item):
    print(f"Processing item: {item.get('o:id')}")
    print(f"Title: {extract_property(item.get('dcterms:title', []), 1)}")
    print(f"Identifier: {extract_property(item.get('dcterms:identifier', []), 10)}")
```

### Common Debugging Techniques

1.  **Inspect API Responses**

    ``` python
    import json

    # Pretty print API responses
    response = requests.get(url)
    print(json.dumps(response.json(), indent=2))
    ```

2.  **Test Individual Functions**

    ``` python
    # Test payload construction
    test_item = {...}  # Sample Omeka item
    payload = construct_payload(test_item, "sgb_OBJECT", "project_iri", [], "", "")
    print(json.dumps(payload, indent=2))
    ```

3.  **Validate Data Transformations**

    ``` python
    # Check property extraction
    props = item.get("dcterms:subject", [])
    subjects = extract_combined_values(props)
    print(f"Extracted subjects: {subjects}")
    ```

### Debug Mode

Add debug mode to main script:

``` python
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'

if DEBUG_MODE:
    # Enable verbose logging
    logging.getLogger().setLevel(logging.DEBUG)
    
    # Add debug breakpoints
    import pdb; pdb.set_trace()
    
    # Save intermediate data
    with open('debug_payload.json', 'w') as f:
        json.dump(payload, f, indent=2)
```

## Code Style {#code-style}

### Python Style Guidelines

Follow PEP 8 with these specific guidelines:

``` python
# Imports
import os
import logging
from typing import Dict, List, Optional

# Constants
MAX_RETRIES = 3
API_TIMEOUT = 30

# Function definitions
def extract_property(props: List[Dict], prop_id: int, as_uri: bool = False) -> str:
    """Extract property value from Omeka property array.
    
    Args:
        props: List of property dictionaries
        prop_id: Numerical property ID to find
        as_uri: Return as formatted URI link
        
    Returns:
        Property value as string or empty string if not found
    """
    for prop in props:
        if prop.get("property_id") == prop_id:
            if as_uri:
                return f"[{prop.get('o:label', '')}]({prop.get('@id', '')})"
            return prop.get("@value", "")
    return ""

# Error handling
try:
    result = api_call()
except requests.RequestException as e:
    logging.error(f"API call failed: {e}")
    raise
```

### Code Formatting Tools

``` bash
# Development dependencies are managed via uv
uv sync --dev  # Installs black, flake8, isort if configured

# Format code
uv run black scripts/
uv run isort scripts/

# Check style
uv run flake8 scripts/

# Pre-commit hooks (configured via pnpm)
pnpm run pre-commit -- run --all-files
```

### Documentation Standards

``` python
def complex_function(param1: str, param2: Optional[Dict] = None) -> List[str]:
    """Brief description of function purpose.
    
    Detailed description if needed. Explain complex logic,
    assumptions, and important behaviors.
    
    Args:
        param1: Description of first parameter
        param2: Description of optional parameter with default behavior
        
    Returns:
        Description of return value and its structure
        
    Raises:
        ValueError: When param1 is invalid
        RequestException: When API calls fail
        
    Example:
        >>> result = complex_function("test", {"key": "value"})
        >>> print(result)
        ['processed', 'values']
    """
    # Implementation here
    pass
```

## Release Process {#release-process}

### Version Management

``` bash
# Update version in setup files
# Follow semantic versioning: MAJOR.MINOR.PATCH

# Tag release
git tag -a v1.2.0 -m "Release version 1.2.0"
git push origin v1.2.0
```

### Release Checklist

-   [ ] All tests pass
-   [ ] Documentation updated
-   [ ] CHANGELOG.md updated
-   [ ] Version numbers updated
-   [ ] Security review completed
-   [ ] Performance benchmarks run
-   [ ] Migration tested on staging

### Deployment

``` bash
# Production deployment checklist
- [ ] Backup current production data
- [ ] Deploy to staging environment
- [ ] Run integration tests
- [ ] Monitor staging for 24 hours
- [ ] Deploy to production
- [ ] Monitor production deployment
```